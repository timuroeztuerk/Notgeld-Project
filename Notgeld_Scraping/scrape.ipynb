{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bt307300\\AppData\\Local\\Temp\\ipykernel_21824\\4000748379.py:17: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  wd = webdriver.Edge(driver_path)\n"
     ]
    }
   ],
   "source": [
    "# This file extracts information about the banknotes from the website gap-banknoten.de and saves it in a json file.\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path for the edge driver.\n",
    "driver_path = \"msedgedriver.exe\"\n",
    "\n",
    "\n",
    "# Define the url of the website.\n",
    "base_url = \"https://www.gap-banknoten.de/deutsches-notgeld-nach-provinzen/wuerttemberg/\"\n",
    "\n",
    "# Call the driver.\n",
    "wd = webdriver.Edge(driver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the product URLs.\n",
    "product_urls = []\n",
    "for page_number in range(1, 32):  # Loop through pages 1 to 31\n",
    "    # Create the URL for the current page\n",
    "    url_path = f\"{base_url}?p={page_number}\"\n",
    "    \n",
    "    # Open the url.\n",
    "    wd.get(url_path)\n",
    "\n",
    "    # Wait for the page to load.\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Get the div class product--info.\n",
    "    product_info = wd.find_elements(By.CLASS_NAME, \"product--info\")\n",
    "    \n",
    "    # Iterate through the product_info elements and extract the product URL\n",
    "    for product in product_info:\n",
    "        product_title = product.find_element(By.CLASS_NAME, \"product--title\")\n",
    "        product_url = product_title.get_attribute(\"href\")\n",
    "        product_urls.append(product_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_product_page(url, webdriver):\n",
    "    # Open the url.\n",
    "    wd.get(url)\n",
    "\n",
    "    # Find the product details container\n",
    "    product_details = webdriver.find_element(By.CLASS_NAME, \"content.product--details\")\n",
    "    \n",
    "    # 1. Extract the product title\n",
    "    product_title = product_details.find_element(By.CLASS_NAME, \"product--title\").text\n",
    "    \n",
    "    # 2. Extract the price content\n",
    "    price_content = product_details.find_element(By.CLASS_NAME, \"price--content.content--default\")\n",
    "    price = float(price_content.find_element(By.TAG_NAME, \"meta\").get_attribute(\"content\"))\n",
    "\n",
    "    # 3. Extract the text content from the base-info--entry elements\n",
    "    base_info_entries = product_details.find_elements(By.CLASS_NAME, \"base-info--entry.entry-attribute\")\n",
    "    entry_contents = []\n",
    "    for entry in base_info_entries:\n",
    "        label = entry.find_element(By.CLASS_NAME, \"entry--label\").text\n",
    "        content = entry.find_element(By.CLASS_NAME, \"entry--content\").text\n",
    "        entry_contents.append((label, content))\n",
    "\n",
    "    # 4. Extract the table content\n",
    "    table_rows = product_details.find_elements(By.CSS_SELECTOR, \".product--properties-table tr\")\n",
    "    table_contents = []\n",
    "    for row in table_rows:\n",
    "        label = row.find_element(By.CLASS_NAME, \"product--properties-label\").text\n",
    "        value = row.find_element(By.CLASS_NAME, \"product--properties-value\").text\n",
    "        table_contents.append((label, value))\n",
    "\n",
    "    # Combine extracted data into a dictionary\n",
    "    product_data = {\n",
    "        'url': url,\n",
    "        'title': product_title,\n",
    "        'price': price,\n",
    "        'base_info': entry_contents,\n",
    "        'table_contents': table_contents\n",
    "    }\n",
    "\n",
    "    return product_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the extracted data from each product page. Takes 30 mins.\n",
    "product_data_list = []\n",
    "\n",
    "# Iterate through the product URLs\n",
    "for product_url in product_urls:\n",
    "    # Open the product URL\n",
    "    wd.get(product_url)\n",
    "    \n",
    "    # Wait for the page to load\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Extract the desired data from the product page\n",
    "    product_data = extract_data_from_product_page(product_url, wd)\n",
    "    \n",
    "    # Append the extracted data to the product_data_list\n",
    "    product_data_list.append(product_data)\n",
    "\n",
    "# Save the product data list to a CSV file\n",
    "csv_columns = ['url', 'title', 'price', 'base_info', 'table_contents']\n",
    "csv_file = \"product_data.csv\"\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "    writer.writeheader()\n",
    "    for data in product_data_list:\n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"product_data.csv\")\n",
    "\n",
    "# Initialize the new columns with empty strings\n",
    "df['Katalog'] = ''\n",
    "df['Erhaltung'] = ''\n",
    "df['Provinz'] = ''\n",
    "df['Wert'] = ''\n",
    "df['Periode'] = ''\n",
    "df['Ort'] = ''\n",
    "\n",
    "# Iterate through the DataFrame rows\n",
    "for index, row in df.iterrows():\n",
    "    # Extract base_info and table_contents as lists of tuples\n",
    "    base_info = eval(row['base_info'])\n",
    "    table_contents = eval(row['table_contents'])\n",
    "\n",
    "    # Find the values in base_info and table_contents\n",
    "    katalog = [content for label, content in base_info if label == 'Katalog-Nr.:']\n",
    "    erhaltung = [value for label, value in table_contents if label == 'Erhaltung:']\n",
    "    provinz = [value for label, value in table_contents if label == 'Provinz:']\n",
    "    wert = [value for label, value in table_contents if label == 'Wert:']\n",
    "    periode = [value for label, value in table_contents if label == 'Periode:']\n",
    "    ort = [value for label, value in table_contents if label == 'Ort:']\n",
    "\n",
    "    # Update the new columns with the extracted values\n",
    "    if katalog:\n",
    "        df.at[index, 'Katalog'] = katalog[0]\n",
    "    if erhaltung:\n",
    "        df.at[index, 'Erhaltung'] = erhaltung[0]\n",
    "    if provinz:\n",
    "        df.at[index, 'Provinz'] = provinz[0]\n",
    "    if wert:\n",
    "        df.at[index, 'Wert'] = wert[0]\n",
    "    if periode:\n",
    "        df.at[index, 'Periode'] = periode[0]\n",
    "    if ort:\n",
    "        df.at[index, 'Ort'] = ort[0]\n",
    "        \n",
    "# Drop the 'base_info' and 'table_contents' columns\n",
    "df = df.drop(columns=['base_info', 'table_contents'])\n",
    "\n",
    "# Reorder the columns\n",
    "df = df[['Ort', 'price','Provinz', 'Wert', 'Periode', 'Erhaltung', 'Katalog', 'url', 'title']]\n",
    "\n",
    "# Save the updated DataFrame to the CSV file\n",
    "df.to_csv(\"product_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"product_data.csv\")\n",
    "\n",
    "# Initialize the new 'date' column with empty strings\n",
    "df['date'] = ''\n",
    "\n",
    "# Define a regex pattern to match date in the format 'dd.mm.yy'\n",
    "date_pattern = re.compile(r'\\b\\d{2}\\.\\d{2}\\.\\d{2}\\b')\n",
    "\n",
    "# Iterate through the DataFrame rows\n",
    "for index, row in df.iterrows():\n",
    "    # Check if there is a date in the title\n",
    "    date_match = date_pattern.search(row['title'])\n",
    "\n",
    "    # If a date is found, save it to the 'date' column\n",
    "    if date_match:\n",
    "        df.at[index, 'date'] = date_match.group()\n",
    "    else:\n",
    "        # If no date is found, fill it with no date.\n",
    "        df.at[index, 'date'] = 'NoDate'\n",
    "\n",
    "# Save the updated DataFrame to the CSV file\n",
    "df.to_csv(\"product_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated DataFrame to the CSV file\n",
    "df.to_csv(\"NotgeldData.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
